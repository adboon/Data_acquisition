\appendix
\chapter{Setup paper 1: Wave-current tank to generate large sets of experimental data}
\label{sec:setup1}

\section*{Abstract}
\label{sec:setup1_abstract}
Large data sets can lead to new insights into old problems. Even though we are able to handle and store large sets of data, for extreme wave impacts on maritime structures like green water and slamming there is not yet a cheap way to generate large sets of data. Large data sets can be obtained cheaply by doing long running, continuous experiments as tank time and man hours are drastically reduced. Creating a testing facility where these long running, continuous experiments with waves and current can be conducted is thus the goal. This test facility is built by equipping an existing current-flumetank with a wavemaker and wave dissipater (beach). The test facility is first validated by reproducing experimental results obtained in a towing tank. Afterwards it is shown that it is able to cheaply generate large data sets.


\section{Introduction}
\label{sec:setup1_intro}
%GEBRUIK MEER ACTIEVE ZINNEN IPV PASSIEVE ZINNEN
The next big shift in scientific methods is happening now, according to \citet{Tolle2011}, \citet{George2016} and \citet{Fricke2015}, as "Big Data'' is suggested to be the fourth paradigm for scientific exploration \texttt{---} the first three are experiments, theories and simulations. This shift comes as over the years computer capabilities have steadily improved. With these improvements we can now store and handle large data sets cheaply, which is thought to go and change scientific methods. \citet{Fricke2015} even asks the question if this might lead to a routine or semi-mechanical way of producing valuable scientific theories.


%In fields like astronomy \cite{Tolle2011}, neuroscience \cite{Choudhury2014} and political science \cite{Monroe2013} the first steps towards this fourth paradigm have been taken.
%With the use of large data sets, new data handling methods, like machine learning, are developed.
\par 
Looking at the field of ship hydromechanics however, it seems large data sets have not made their introduction. There are some examples of the use of big data in the offshore engineering for, for example, route optimization \cite{Yan2018,Zhang2018}, ship traffic and collision evaluation \cite{Wang2020a,Kang2018, Zhang2021,Zhang2018,Zhang2019b}, performance monitoring \cite{Bui2021} or maintenance cycles \cite{Shaw2021}.  
%EXAMPLES WITH SIMULATIONS AND MAYBE OTHER DATA SETS. VOORBEELDEN TOEVOEGEN. MOGELIJK UIT HET OVERZICHT VAN MACHINE LEARNING IN MT. 
These examples show that there is some use, but not for the hydromechanics of a ship. \par 
Large data sets provide new ways of viewing a problem, giving new insights into old problems.  
Looking at for instance extreme nonlinear wave impacts like sloshing and green water. There is no method to model them yet. The problem is complex, with a statistical component to these impacts, with variability within 'constant' conditions \cite{Bullock2007,Bogaert2010,Bredmose2009,Peregrine2003,Abdussamie2017}. Also many parameters and nonlinear effects are at play \cite{Greco2001,Buchner2002, Faltinsen2002} and the events do not occur often as they are rare events.
\par
For complex problems like sloshing and green water, experiments are needed as they can not yet be accurately and affordably be simulated \cite{Soares2005}. 
It is an option to drive up and down many times in a towing tank and then knit all the runs together to obtain a large data set. However, the man hours, tank time and post-processing needed are costly. This, in turn, limits the amount of data that can be obtained.\par

\subsection*{Problem statement}
\label{sec:problem}
This shows the problem: even though we are able to handle and store large sets of data, we can not yet cheaply generate large sets of experimental data in a controlled environment. This means we are missing out on a new way of doing research.
To generate the required data, we need to limit the amount of tank time and man hours needed to generate data. This can be achieved by doing long running, continuous experiments. This way the waiting time, in which you wait till all the waves and disturbances are dissipated, in between runs is eliminated. This significantly reduces the time the tank is in use, and also the hours of manpower needed. 
There are tanks where you could do long running, continuous experiments with waves \textit{or} currents in wave- or current-flumetanks.  In wave-flumetanks there is no current. Without the current, waves are free to flow back after reflecting behind the model, disturbing incoming waves, which in turn reduces the quality of the experiments. Also, no forward velocity or current flow can be modeled in these tanks. In current-flumetanks, there are no waves. This is problematic as, as discussed before, the use of large data sets would be interesting mostly for extreme wave impacts. \\
What is needed is a testing facility where long running, continuous experiments with waves and current can be conducted.\\ \\
\textbf{Goal}: 
A new testing facility that can cheaply generate large sets of data by allowing continuous long running experiments with waves and current.

% of the same quality as a towing tank.
% This is interesting as we can already cheaply handle and save large sets of data and now also cheaply generate them for situations with waves and current. Research topics where this could help with insights are extreme nonlinear wave impacts like slamming and green water impacts.

\section{Method}
\label{sec:method}
Adapting the existing flumetank to create a wave-current tank, capable of continuously generating waves and currents. This allows for long running experiments \texttt{---} up to hours or days. This will be done by converting an existing current-flumetank to a wave-current tank by placing a wave generating device (wavemaker) and wave dissipating device (beach) on it. The new testing facility will be validate by comparing the same experiments from the towing tank with the results obtained from the new wave-current tank. Afterwards long experiments will be conducted to verified that it can indeed also generate large data sets. 

%
%Monroe 2013, the 5 V's%Interesting from \cite{Monroe2013}: We need to consider volume (amount of data), velocity (working with data within a reasonable time span), variability (variety of contexts (or methods) to see the data in), vinculation (connections) and validity (how do you check if what you got makes sense and is according to expactation)
%
%
%Fricke 2015, structure arguments%INTERESTING POINT OF VIEW, MIGHT BE HELPFULL TO STRUCTURE ARGUMENTS "Science made a great leap forward with the advent of the experimental method (in the modern era, roughly from Hooke onward, ˜1660). What is so special about this? We are looking for lawlike or nomological connections to connect causes with effects. But what we observe can be misleading due to confounds. Confounds are other condi- tions or variables which correlate either with the causes or with the effects to mask what is really happening at a causal level. The experimental method is, in part, a tech- nique to deliberately manipulate nature so that known pos- sible confounds are controlled for. With the experimental method, we ask nature questions which are deliberately framed. With a typical scientific problem and a tentative hypothesis offered to solve it, there are known confounds and also maybe unknown confounds. The known con- founds are controlled for. There is a view on experimental technique, the standard view from Fisher, that the problem of unknown confounds can be addressed by randomization, for example, using randomized controlled trials (RCTs). Actually, it seems that randomization via RCTs is not nec- essary (Urbach, 1985; Worrall, 2007); however attention certainly does need to be paid to potential confounds, both known and unknown." \cite{Fricke2015} 
%Fricke 2015, his conclusion %"The ability to cheaply and easily gather large amounts of data does have advantages: Sample sizes can be larger, testing of theories can be better, there can be continuous assessment, and so on. But data-driven science, the “fourth paradigm,” is a chimera. Science needs problems, thoughts, theories, and designed experiments. If anything, science needs more theories and less data."